{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-cmG4MzqyjE.npy', '0EZaw5V-LfU.npy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List audio files\n",
    "audio_files_lst = os.listdir(\"/home/alexandertchk/VSCode/multimodal/how2-dataset/audio_300/concat\")\n",
    "audio_files_lst[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-4j8fghNOE.npy', '00DKFksyVnQ.npy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List video files\n",
    "video_files_lst = os.listdir(\"/home/alexandertchk/VSCode/multimodal/how2-dataset/video_action_features\")\n",
    "video_files_lst[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare audio/video equality\n",
    "audio_files_lst == video_files_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The source and destination paths\n",
    "source_folder_path = \"/home/alexandertchk/VSCode/multimodal/how2-dataset/en_sum/text\"\n",
    "destination_folder_path = \"/home/alexandertchk/VSCode/multimodal/dataset\"\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(destination_folder_path, exist_ok=True)\n",
    "\n",
    "# Your list of audio file names, converted to video IDs\n",
    "video_ids = [file.split('.')[0] for file in audio_files_lst]\n",
    "\n",
    "# Sub folders inside source_folder_path\n",
    "sub_folders = ['sum_cv', 'sum_devtest', 'sum_train']\n",
    "\n",
    "# Open destination files\n",
    "with open(os.path.join(destination_folder_path, 'desc.tok.txt'), 'w') as desc_out, \\\n",
    "     open(os.path.join(destination_folder_path, 'tran.tok.txt'), 'w') as tran_out:\n",
    "\n",
    "    # Loop through each video id\n",
    "    for video_id in video_ids:\n",
    "        # Loop through each subfolder\n",
    "        for sub_folder in sub_folders:\n",
    "            # Construct the file paths\n",
    "            desc_file_path = os.path.join(source_folder_path, sub_folder, 'desc.tok.txt')\n",
    "            tran_file_path = os.path.join(source_folder_path, sub_folder, 'tran.tok.txt')\n",
    "\n",
    "            # Loop through each line in the desc and tran files, and write to output if it starts with the video_id\n",
    "            with open(desc_file_path, 'r') as desc_in:\n",
    "                for line in desc_in:\n",
    "                    if line.startswith(video_id):\n",
    "                        desc_out.write(line)\n",
    "\n",
    "            with open(tran_file_path, 'r') as tran_in:\n",
    "                for line in tran_in:\n",
    "                    if line.startswith(video_id):\n",
    "                        tran_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_and_delete_non_corresponding_files(audio_path, txt_path):\n",
    "    audio_files = [f for f in os.listdir(audio_path) if f.endswith('.npy')]\n",
    "    tran_tok_file = os.path.join(txt_path, 'tran.tok.txt')\n",
    "    desc_tok_file = os.path.join(txt_path, 'desc.tok.txt')\n",
    "    tran_lines = []\n",
    "    desc_lines = []\n",
    "\n",
    "    # Read tran.tok.txt and desc.tok.txt files into lists\n",
    "    with open(tran_tok_file, 'r') as tran_file:\n",
    "        tran_lines = tran_file.readlines()\n",
    "    \n",
    "    with open(desc_tok_file, 'r') as desc_file:\n",
    "        desc_lines = desc_file.readlines()\n",
    "\n",
    "    # Create sets to store video IDs from npy files and text files\n",
    "    npy_video_ids = set(os.path.splitext(f)[0] for f in audio_files)\n",
    "    txt_video_ids = set(line.split(' ')[0] for line in tran_lines)\n",
    "\n",
    "    # Find the intersection of video IDs between npy files and text files\n",
    "    common_video_ids = npy_video_ids.intersection(txt_video_ids)\n",
    "\n",
    "    # Filter lines that have corresponding npy files\n",
    "    tran_lines_filtered = [line for line in tran_lines if line.split(' ')[0] in common_video_ids]\n",
    "    desc_lines_filtered = [line for line in desc_lines if line.split(' ')[0] in common_video_ids]\n",
    "\n",
    "    # Filter npy files that have corresponding lines in both tran.tok.txt and desc.tok.txt\n",
    "    npy_files_filtered = [f for f in audio_files if os.path.splitext(f)[0] in common_video_ids]\n",
    "\n",
    "    # Write the filtered lines back to tran.tok.txt and desc.tok.txt\n",
    "    with open(tran_tok_file, 'w') as tran_file:\n",
    "        tran_file.writelines(tran_lines_filtered)\n",
    "    \n",
    "    with open(desc_tok_file, 'w') as desc_file:\n",
    "        desc_file.writelines(desc_lines_filtered)\n",
    "\n",
    "    # Delete npy files that don't have corresponding lines\n",
    "    for npy_file in audio_files:\n",
    "        video_id = os.path.splitext(npy_file)[0]\n",
    "        if video_id not in common_video_ids:\n",
    "            npy_path = os.path.join(audio_path, npy_file)\n",
    "            os.remove(npy_path)\n",
    "\n",
    "    return npy_files_filtered\n",
    "\n",
    "# Example usage:\n",
    "audio_path = '/home/alexandertchk/VSCode/multimodal/how2-dataset/audio_300/concat'\n",
    "txt_path = '/home/alexandertchk/VSCode/multimodal/dataset/'\n",
    "\n",
    "filtered_npy_files = check_and_delete_non_corresponding_files(audio_path, txt_path)\n",
    "print(len(filtered_npy_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the text data from dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataset_path, train_percent, cv_percent, test_percent):\n",
    "    # Assert that the provided percentages add up to 100\n",
    "    assert train_percent + cv_percent + test_percent == 1.0, \"Provided percentages don't add up to 1.0\"\n",
    "\n",
    "    # Define the file paths\n",
    "    desc_file_path = os.path.join(dataset_path, 'desc.tok.txt')\n",
    "    tran_file_path = os.path.join(dataset_path, 'tran.tok.txt')\n",
    "\n",
    "    # Load the files\n",
    "    with open(desc_file_path, 'r') as file:\n",
    "        desc_lines = file.readlines()\n",
    "\n",
    "    with open(tran_file_path, 'r') as file:\n",
    "        tran_lines = file.readlines()\n",
    "\n",
    "    # Split the data into train, cv and test\n",
    "    desc_train, desc_temp, tran_train, tran_temp = train_test_split(desc_lines, tran_lines, test_size=1-train_percent, random_state=42)\n",
    "    test_ratio = test_percent / (cv_percent + test_percent)\n",
    "    desc_cv, desc_test, tran_cv, tran_test = train_test_split(desc_temp, tran_temp, test_size=test_ratio, random_state=42)\n",
    "\n",
    "    # Create the destination folders and files\n",
    "    subsets = ['sum_train_300', 'sum_cv_300', 'sum_devtest_300']\n",
    "    data = [(desc_train, tran_train), (desc_cv, tran_cv), (desc_test, tran_test)]\n",
    "    for subset, (desc_data, tran_data) in zip(subsets, data):\n",
    "        subset_folder = os.path.join(dataset_path, subset)\n",
    "        os.makedirs(subset_folder, exist_ok=True)\n",
    "        with open(os.path.join(subset_folder, 'desc.tok.txt'), 'w') as file:\n",
    "            file.writelines(desc_data)\n",
    "        with open(os.path.join(subset_folder, 'tran.tok.txt'), 'w') as file:\n",
    "            file.writelines(tran_data)\n",
    "\n",
    "# Use the function\n",
    "dataset_path = \"/home/alexandertchk/VSCode/multimodal/dataset\"\n",
    "split_data(dataset_path, train_percent=0.70, cv_percent=0.15, test_percent=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get video_ids for each split\n",
    "folders = ['sum_train_300', 'sum_devtest_300', 'sum_cv_300']\n",
    "base_path = '/home/alexandertchk/VSCode/multimodal/dataset'\n",
    "\n",
    "for folder in folders:\n",
    "    input_file = os.path.join(base_path, folder, 'desc.tok.txt')\n",
    "    output_file = os.path.join(base_path, folder, 'id.txt')\n",
    "\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            video_id, _ = line.strip().split(' ', 1)\n",
    "            f_out.write(video_id + '\\n')#Copy audio and video files from audio concat and bideo\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths to the source and destination folders\n",
    "source_action_folder = \"/home/alexandertchk/VSCode/multimodal/how2-dataset/video_action_features\"\n",
    "source_audio_folder = \"/home/alexandertchk/VSCode/multimodal/how2-dataset/audio_300/concat\"\n",
    "\n",
    "dest_folders = {\n",
    "    \"train\": {\n",
    "        \"action\": \"/home/alexandertchk/VSCode/multimodal/dataset/actions_train_300\",\n",
    "        \"audio\": \"/home/alexandertchk/VSCode/multimodal/dataset/audio_train_300\",\n",
    "    },\n",
    "    \"cv\": {\n",
    "        \"action\": \"/home/alexandertchk/VSCode/multimodal/dataset/actions_cv_300\",\n",
    "        \"audio\": \"/home/alexandertchk/VSCode/multimodal/dataset/audio_cv_300\",\n",
    "    },\n",
    "    \"devtest\": {\n",
    "        \"action\": \"/home/alexandertchk/VSCode/multimodal/dataset/actions_devtest_300\",\n",
    "        \"audio\": \"/home/alexandertchk/VSCode/multimodal/dataset/audio_devtest_300\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Read video_ids from txt files\n",
    "def read_video_ids(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# Function to copy files from source to destination\n",
    "def copy_files(video_ids, source_folder, dest_folder):\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    for video_id in video_ids:\n",
    "        action_file = os.path.join(source_action_folder, f\"{video_id}.npy\")\n",
    "        audio_file = os.path.join(source_audio_folder, f\"{video_id}.npy\")\n",
    "        if os.path.exists(action_file):\n",
    "            shutil.copy(action_file, dest_folder)\n",
    "        if os.path.exists(audio_file):\n",
    "            shutil.copy(audio_file, dest_folder)\n",
    "\n",
    "# Read video_ids from each txt file and copy corresponding files\n",
    "for split, dest_paths in dest_folders.items():\n",
    "    train_file_path = f\"/home/alexandertchk/VSCode/multimodal/dataset/{split}_id.txt\"\n",
    "    video_ids = read_video_ids(train_file_path)\n",
    "    copy_files(video_ids, source_action_folder, dest_paths[\"action\"])\n",
    "    copy_files(video_ids, source_audio_folder, dest_paths[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sum_train_300', 'sum_devtest_300', 'sum_cv_300', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_txt_files_audio_video(root_folder):\n",
    "    # Get the list of directories in root_folder\n",
    "    directories = ['actions_cv_300',]\n",
    "\n",
    "    # Loop over each directory\n",
    "    for directory in directories:\n",
    "        txt_file_path = os.path.join(root_folder, directory, directory + '.txt')\n",
    "        # Open the .txt file for writing\n",
    "        with open(txt_file_path, 'w') as txt_file:\n",
    "            # Get the list of .npy files in the directory\n",
    "            npy_files = [f for f in os.listdir(os.path.join(root_folder, directory)) if f.endswith('.npy')]\n",
    "            # Loop over each .npy file\n",
    "            for npy_file in npy_files:\n",
    "                # Write the video_id and the path to the .npy file to the .txt file\n",
    "                video_id = npy_file.split('.')[0]\n",
    "                npy_file_path = os.path.join(root_folder, directory, npy_file)\n",
    "                txt_file.write('npy_file_path\\n')\n",
    "\n",
    "root_folder = \"/home/alexandertchk/VSCode/multimodal/dataset\"\n",
    "create_txt_files_audio_video(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# If you have not already downloaded the NLTK tokenization package, uncomment the following line to do so:\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def generate_vocab_file(input_file_path, output_file_path):\n",
    "    # Read the input file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Tokenize the text and count the occurrence of each token\n",
    "    token_counts = Counter()\n",
    "    for line in lines:\n",
    "        # The first token of each line is the video id, which we don't want to include in the vocabulary\n",
    "        text = line.split(' ', 1)[1]\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "    # Write the tokens and their counts to the output file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for token, count in token_counts.items():\n",
    "            file.write(f'{token} {count}\\n')\n",
    "\n",
    "input_file_path = '/home/alexandertchk/VSCode/multimodal/how2-dataseten_sum/text/sum_train/tran.tok.txt'\n",
    "output_file_path = '/home/alexandertchk/VSCode/multimodal/how2-datasetdataset/tran.tok.vocab.txt'\n",
    "generate_vocab_file(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'audio_devtest_300.txt' in folder 'audio_devtest_300'\n",
      "Created 'actions_train_300.txt' in folder 'actions_train_300'\n",
      "Created 'audio_train_300.txt' in folder 'audio_train_300'\n",
      "Created 'actions_devtest_300.txt' in folder 'actions_devtest_300'\n",
      "Created 'audio_cv_300.txt' in folder 'audio_cv_300'\n",
      "Created 'actions_cv_300.txt' in folder 'actions_cv_300'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the list of folders\n",
    "folders = [\n",
    "    'audio_devtest_300',\n",
    "    'actions_train_300',\n",
    "    'audio_train_300',\n",
    "    'actions_devtest_300',\n",
    "    'audio_cv_300',\n",
    "    'actions_cv_300'\n",
    "]\n",
    "\n",
    "# Define the base directory path\n",
    "base_directory = '/home/alexandertchk/VSCode/multimodal/dataset'\n",
    "\n",
    "# Loop through each folder\n",
    "for folder_name in folders:\n",
    "    # Create the full path to the current folder\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "\n",
    "    # Get a list of all files in the current folder using glob\n",
    "    files_list = glob.glob(os.path.join(folder_path, '*'))\n",
    "\n",
    "    # Create the text file name by appending '.txt' to the folder name\n",
    "    txt_file_name = folder_name + '.txt'\n",
    "\n",
    "    # Create and write the file list to the text file\n",
    "    with open(os.path.join(folder_path, txt_file_name), 'w') as txt_file:\n",
    "        txt_file.write('\\n'.join(files_list))\n",
    "\n",
    "    print(f\"Created '{txt_file_name}' in folder '{folder_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alexandertchk/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# If you have not already downloaded the NLTK tokenization package, uncomment the following line to do so:\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def generate_vocab_file(input_file_path, output_file_path):\n",
    "    # Read the input file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Tokenize the text and count the occurrence of each token\n",
    "    token_counts = Counter()\n",
    "    for line in lines:\n",
    "        # The first token of each line is the video id, which we don't want to include in the vocabulary\n",
    "        text = line.split(' ', 1)[1]\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "    # Write the tokens and their counts to the output file in JSON format\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(dict(token_counts), file)\n",
    "\n",
    "input_file_path = '/home/alexandertchk/VSCode/multimodal/how2-dataset/en_sum/text/sum_train/desc.tok.txt'\n",
    "output_file_path = '/home/alexandertchk/VSCode/multimodal/dataset/desc.tok.vocab.json'\n",
    "generate_vocab_file(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
